{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bhashini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bhashini_translator import Bhashini\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhashini_translate(sourceLanguage, targetLanguage, text):\n",
    "    bhashini = Bhashini(sourceLanguage, targetLanguage)\n",
    "    return bhashini.translate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "роОрокрпНрокроЯро┐ роЗро░рпБроХрпНроХрпАроЩрпНроХ?\n"
     ]
    }
   ],
   "source": [
    "print(bhashini_translate('en','ta', 'How are you?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =\"You are an Educational Assistant. The user may ask about various educational topics, technical queries, and other concerns. Respond in a detailed but simple manner. for example the user ask a query like of  this 'What is the boiling point of water?' you should respond in the form of like this 'The boiling point of water is 100┬░C (212┬░F) at sea level. This is the temperature at which water changes from a liquid to a gas'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def LLM(query, content):\n",
    "# Point to the local server\n",
    "    client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "    passLLM = f'Question:{query}\\nContent:{content}'\n",
    "    #print(passLLM)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an Educational Assistant. The user may ask about various educational topics, technical queries, and other concerns. Respond in a detailed but simple manner. for example the user ask a query like of  this 'What is the boiling point of water?' you should respond in the form of like this 'The boiling point of water is 100┬░C (212┬░F) at sea level. This is the temperature at which water changes from a liquid to a gas'.\"},\n",
    "        {\"role\": \"user\", \"content\": passLLM}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    )\n",
    "\n",
    "    print(completion.choices[0].message)\n",
    "    return completion.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45273"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Open the PDF file\n",
    "doc = fitz.open(\"D:/Samaaja/Vil-Bert.pdf\")\n",
    "\n",
    "# Extract text from each page\n",
    "pdf_text = \"\"\n",
    "for page in doc:\n",
    "    pdf_text += page.get_text()\n",
    "\n",
    "len(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_Splitter = RecursiveCharacterTextSplitter(chunk_size= 1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_Chunks = text_Splitter.create_documents([pdf_text])\n",
    "text_Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [text_Chunk.page_content for text_Chunk in text_Chunks]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without Built-in\n",
    "def recursive_text_splitter(text, max_chunk_size=1000, overlap_size=200):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(\" \".join(current_chunk)) >= max_chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = current_chunk[-overlap_size:]  # retain overlap words\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = recursive_text_splitter(pdf_text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import spacy\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"multilingualedtechrag\")\n",
    "#nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(documents=text, ids = [str(i) for i in range(len(text))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is VilBERT is Parallel or Single Stream? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_texts=[question], n_results=4)\n",
    "answer = results['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['network depth for each modality and enables cross-modal connections at different depths.\\nOur model which we call ViLBERT is shown in Fig. 1 and consists of two parallel BERT-style\\nmodels operating over image regions and text segments. Each stream is a series of transformer\\nblocks (TRM) and novel co-attentional transformer layers (Co-TRM) which we introduce to enable\\ninformation exchange between modalities. Given an image I represented as a set of region features\\nv1, . . . , vT and a text input w0, . . . , wT , our model outputs ямБnal representations hv0, . . . , hvT and\\nhw0, . . . , hwT . Notice that exchange between the two streams is restricted to be between speciямБc\\n1Concurrent work [29] modelling language and video sequences takes this approach. See Sec. 5.\\n3\\nVision               &        Language BERT\\nтАж\\nтДО\"0\\nтДО$%\\nтДО$&\\nтДО$\\'\\nтДО$ЁЭТп\\n<IMG>\\nтДО)0\\nтДО*%\\nтДО*&\\nтДО*\\'\\nтДО*+\\n<CLS>\\nMan\\nshopping\\nfor\\n<SEP>\\nтАж\\nтАж\\nтАж\\n<MASK>\\n<MASK>\\n<MASK>\\n<MASK>\\nMan shopping\\n(a) Masked multi-modal learning',\n",
       " 'VCR and VQA which have private test sets, we report test results (in parentheses) only for our full\\nmodel. Our full ViLBERT model outperforms task-speciямБc state-of-the-art models across all tasks.\\nVQA [3]\\nVCR [25]\\nRefCOCO+ [32]\\nImage Retrieval [26]\\nZS Image Retrieval\\nMethod\\ntest-dev (test-std)\\nQтЖТA\\nQAтЖТR\\nQтЖТAR\\nval\\ntestA\\ntestB\\nR1\\nR5\\nR10\\nR1\\nR5\\nR10\\nSOTA\\nDFAF [36]\\n70.22 (70.34)\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nR2C [25]\\n-\\n63.8 (65.1)\\n67.2 (67.3)\\n43.1 (44.0)\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nMAttNet [33]\\n-\\n-\\n-\\n-\\n65.33\\n71.62\\n56.02\\n-\\n-\\n-\\n-\\n-\\n-\\nSCAN [35]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n48.60\\n77.70\\n85.20\\n-\\n-\\n-\\nOurs\\nSingle-StreamтАа\\n65.90\\n68.15\\n68.89\\n47.27\\n65.64\\n72.02\\n56.04\\n-\\n-\\n-\\n-\\n-\\n-\\nSingle-Stream\\n68.85\\n71.09\\n73.93\\n52.73\\n69.21\\n75.32\\n61.02\\n-\\n-\\n-\\n-\\n-\\n-\\nViLBERTтАа\\n68.93\\n69.26\\n71.01\\n49.48\\n68.61\\n75.97\\n58.44\\n45.50\\n76.78\\n85.02\\n0.00\\n0.00\\n0.00\\nViLBERT\\n70.55 (70.92)\\n72.42 (73.3)\\n74.47 (74.6)\\n54.04 (54.8)\\n72.34\\n78.52\\n62.61\\n58.20\\n84.90\\n91.52\\n31.86\\n61.12\\n72.80\\nTask-SpeciямБc Baselines. To put our results in context, we present published results of problem-',\n",
       " 'What does ViLBERT learn during pretraining? To get a sense for what ViLBERT learns dur-\\ning Conceptual Caption pretraining, we look at zero-shot caption-based image retreival and some\\nqualitative examples. While zero-shot performance (Tab. 1, right) is signiямБcantly lower than the\\nямБne-tuned model (31.86 vs 58.20 R1) it performs reasonably without having seen a Flickr30k image\\nor caption (31.86 vs 48.60 R1 for prior SOTA) тАУ indicating that ViLBERT has learned a semantically\\nmeaningful alignment between vision and language during pretraining. We also qualitatively inspect\\nthe pretrained ViLBERT model by inputting images and sampling out image-conditioned text. This\\nis essentially image captioning. Note that without ямБne-tuning the model on clean, human-annotated\\ncaptioning data, the outputs are not likely to be high quality. However, they still serve as a mechanism\\nto inspect what the pretrained model has learned. Fig. 5 shows some of these sampled тАШcaptionsтАЩ.',\n",
       " 'Overall, these results demonstrate that our ViLBERT model is able to learn important visual-linguistic\\nrelationships that can be exploited by downstream tasks.\\nEffect of Visual Stream Depth. In Tab. 2 we compare the results transferring from ViLBERT models\\nof varying depths. We consider depth with respect to the number of repeated CO-TRMтЖТTRM blocks\\n(shown in a dashed box in Fig. 1) in our model. We ямБnd that VQA and Image Retrieval tasks\\nbeneямБt from greater depth - performance increases monotonically until a layer depth of 6. Likewise,\\nzero-shot image retrieval continues making signiямБcant gains as depth increases. In contrast, VCR and\\nRefCOCO+ seem to beneямБt from shallower models.\\nBeneямБts of Large Training Sets. We also studied the impact of the size of the pretraining dataset.\\nFor this experiment, we take random subsets of 25% and 50% from the conceptual caption dataset,\\nand pretrain and ямБnetune ViLBERT using the same setup as above. We can see that the accuracy']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ViLBERTisaparallelstreammodelmeaningitconsistsoftwoseparatestreamsofBERTstylemodelsoperatinginparallelEachstreamhasitsownseriesoftransformerblocksandcoattentionaltransformerlayerswhichenableinformationexchangebetweenmodalitiesTheinputimageisrepresentedasasetofregionfeaturesandtheinputtextisrepresentedasasequenceofwordsThefinalrepresentationsfrombothstreamsarecombinedandthemodeloutputsanalrepresentationTheViLBERTmodellearnsimportantvisuallinguisticrelationshipsduringpretrainingasdemonstratedbyitsperformanceonzeroshotcaptionbasedimageretrievaltasksThemodelsabilitytotransferknowledgetodownstreamtasksisevidentinitsperformanceacrossvarioustasksincludingVQAImageRetrievalandRefCOCOTheeffectofvisualstreamdepthisalsostudiedanditisfoundthatbothVQAandImageRetrievaltasksbenetfromgreaterdepthwhileVCRandRefCOCOseemtobenetfromshallowermodelsFinallytheimpactofthesizeofthepretrainingdatasetisexploredanditisfoundthatlargertrainingsetsresultinbetterperformance'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_and_extract_words(text):\n",
    "    words = re.findall(r'[A-Za-z]+', text)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def clean_segments_and_print(answer):\n",
    "    cleaned_content = []\n",
    "\n",
    "    for segment in answer:\n",
    "        cleaned_segment = clean_and_extract_words(segment)\n",
    "        cleaned_content.append(cleaned_segment)\n",
    "    seg = \"\"\n",
    "    for i, segment in enumerate(cleaned_content, start=1):\n",
    "        #print(f\"Segment {i}:\")\n",
    "        seg += segment\n",
    "        #print(segment)\n",
    "        #print(\"\\n\")\n",
    "    return seg\n",
    "\n",
    "cleaned_text = clean_segments_and_print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"ViLBERT is a parallel stream model, meaning it consists of two separate streams of BERT-style models operating in parallel. Each stream has its own series of transformer blocks and co-attentional transformer layers, which enable information exchange between modalities. The input image is represented as a set of region features, and the input text is represented as a sequence of words. The final representations from both streams are combined, and the model outputs a ямБnal representation.\\nThe ViLBERT model learns important visual-linguistic relationships during pretraining, as demonstrated by its performance on zero-shot caption-based image retrieval tasks. The model's ability to transfer knowledge to downstream tasks is evident in its performance across various tasks, including VQA, Image Retrieval, and RefCOCO+. The effect of visual stream depth is also studied, and it is found that both VQA and Image Retrieval tasks beneямБt from greater depth, while VCR and RefCOCO+ seem to beneямБt from shallower models. Finally, the impact of the size of the pretraining dataset is explored, and it is found that larger training sets result in better performance.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "response = LLM(question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_llm =\"ViLBERT is a parallel stream model, meaning it consists of two separate streams of BERT-style models operating in parallel. Each stream has its own series of transformer blocks and co-attentional transformer layers, which enable information exchange between modalities. The input image is represented as a set of region features, and the input text is represented as a sequence of words. The final representations from both streams are combined, and the model outputs a ямБnal representation.\\nThe ViLBERT model learns important visual-linguistic relationships during pretraining, as demonstrated by its performance on zero-shot caption-based image retrieval tasks. The model's ability to transfer knowledge to downstream tasks is evident in its performance across various tasks, including VQA, Image Retrieval, and RefCOCO+. The effect of visual stream depth is also studied, and it is found that both VQA and Image Retrieval tasks beneямБt from greater depth, while VCR and RefCOCO+ seem to beneямБt from shallower models. Finally, the impact of the size of the pretraining dataset is explored, and it is found that larger training sets result in better performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ViLBERT is a parallel stream model, meaning it consists of two separate streams of BERT-style models operating in parallel. Each stream has its own series of transformer blocks and co-attentional transformer layers, which enable information exchange between modalities. The input image is represented as a set of region features, and the input text is represented as a sequence of words. The final representations from both streams are combined, and the model outputs a ямБnal representation.\\nThe ViLBERT model learns important visual-linguistic relationships during pretraining, as demonstrated by its performance on zero-shot caption-based image retrieval tasks. The model's ability to transfer knowledge to downstream tasks is evident in its performance across various tasks, including VQA, Image Retrieval, and RefCOCO+. The effect of visual stream depth is also studied, and it is found that both VQA and Image Retrieval tasks beneямБt from greater depth, while VCR and RefCOCO+ seem to beneямБt from shallower models. Finally, the impact of the size of the pretraining dataset is explored, and it is found that larger training sets result in better performance\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VILBERT роОройрпНрокродрпБ роТро░рпБ роЗрогрпИропро╛рой ро╕рпНроЯрпНро░рпАроорпН рооро╛родро┐ро░ро┐ропро╛роХрпБроорпН, роЕродро╛ро╡родрпБ роЗродрпБ роЗрогрпИропро╛роХ роЪрпЖропро▓рпНрокроЯрпБроорпН BERT-рокро╛рогро┐ рооро╛родро┐ро░ро┐роХро│ро┐ройрпН роЗро░рогрпНроЯрпБ родройро┐родрпНродройро┐ ро╕рпНроЯрпНро░рпАроорпНроХро│рпИроХрпН роХрпКрогрпНроЯрпБро│рпНро│родрпБ. роТро╡рпНро╡рпКро░рпБ ро╕рпНроЯрпНро░рпАроорпН роЕродройрпН роЪрпКроирпНрод родрпКроЯро░рпН рооро┐ройрпНрооро╛ро▒рпНро▒ро┐ родрпКроХрпБродро┐роХро│рпН рооро▒рпНро▒рпБроорпН роЗрогрпИ роХро╡ройроорпН рооро┐ройрпНрооро╛ро▒рпНро▒ро┐ роЕроЯрпБроХрпНроХрпБроХро│рпИроХрпН роХрпКрогрпНроЯрпБро│рпНро│родрпБ, роЗродрпБ роорпБро▒рпИроХро│рпБроХрпНроХрпБ роЗроЯрпИропро┐ро▓рпН родроХро╡ро▓рпН рокро░ро┐рооро╛ро▒рпНро▒родрпНродрпИ роЪрпЖропро▓рпНрокроЯрпБродрпНродрпБроХро┐ро▒родрпБ. роЙро│рпНро│рпАроЯрпНроЯрпБ рокроЯроорпН рокро┐ро░ро╛роирпНродро┐роп роЕроорпНроЪроЩрпНроХро│ро┐ройрпН родрпКроХрпБрокрпНрокро╛роХ роХрпБро▒ро┐рокрпНрокро┐роЯрокрпНрокроЯрпБроХро┐ро▒родрпБ, роорпЗро▓рпБроорпН роЙро│рпНро│рпАроЯрпНроЯрпБ роЙро░рпИ роЪрпКро▒рпНроХро│ро┐ройрпН ро╡ро░ро┐роЪрпИропро╛роХ роХрпБро▒ро┐рокрпНрокро┐роЯрокрпНрокроЯрпБроХро┐ро▒родрпБ. роЗро░рогрпНроЯрпБ роирпАро░рпЛроЯрпИроХро│ро┐ро▓ро┐ро░рпБроирпНродрпБроорпН роЗро▒рпБродро┐ рокро┐ро░родро┐роиро┐родро┐родрпНродрпБро╡роЩрпНроХро│рпН роЗрогрпИроХрпНроХрокрпНрокроЯрпБроХро┐ройрпНро▒рой, роорпЗро▓рпБроорпН рооро╛родро┐ро░ро┐ роТро░рпБ роЗро▒рпБродро┐ рокро┐ро░родро┐роиро┐родро┐родрпНродрпБро╡родрпНродрпИ ро╡рпЖро│ро┐ропро┐роЯрпБроХро┐ро▒родрпБ. VILBERT рооро╛родро┐ро░ро┐ роорпБройрпН рокропро┐ро▒рпНроЪро┐ропро┐ройрпН рокрпЛродрпБ роорпБроХрпНроХро┐ропрооро╛рой роХро╛роЯрпНроЪро┐-роорпКро┤ро┐ропро┐ропро▓рпН роЙро▒ро╡рпБроХро│рпИроХрпН роХро▒рпНро▒рпБроХрпНроХрпКро│рпНроХро┐ро▒родрпБ, роЗродрпБ рокрпВроЬрпНроЬро┐роп-ро╖ро╛роЯрпН родро▓рпИрокрпНрокрпБ-роЕроЯро┐рокрпНрокроЯрпИропро┐ро▓ро╛рой рокроЯ роорпАроЯрпНроЯрпЖроЯрпБрокрпНрокрпБ рокрогро┐роХро│ро┐ро▓рпН роЕродройрпН роЪрпЖропро▓рпНродро┐ро▒ройро╛ро▓рпН роиро┐ро░рпВрокро┐роХрпНроХрокрпНрокроЯрпНроЯрпБро│рпНро│родрпБ. роХрпАро┤рпНроиро┐ро▓рпИрокрпН рокрогро┐роХро│рпБроХрпНроХрпБ роЕро▒ро┐ро╡рпИ рооро╛ро▒рпНро▒рпБроорпН рооро╛родро┐ро░ро┐ропро┐ройрпН родро┐ро▒ройрпН VQA, рокроЯ роорпАроЯрпНроЯрпЖроЯрпБрокрпНрокрпБ рооро▒рпНро▒рпБроорпН ро░рпЖроГрокрпНроХрпЛроХрпЛ + роЙро│рпНро│ро┐роЯрпНроЯ рокро▓рпНро╡рпЗро▒рпБ рокрогро┐роХро│ро┐ро▓рпН роЕродройрпН роЪрпЖропро▓рпНродро┐ро▒ройро┐ро▓рпН родрпЖро│ро┐ро╡ро╛роХродрпН родрпЖро░ро┐роХро┐ро▒родрпБ. роХро╛роЯрпНроЪро┐ ро╕рпНроЯрпНро░рпАроорпН роЖро┤родрпНродро┐ройрпН ро╡ро┐ро│рпИро╡рпБроорпН роЖропрпНро╡рпБ роЪрпЖропрпНропрокрпНрокроЯрпБроХро┐ро▒родрпБ, роорпЗро▓рпБроорпН ро╡ро┐. роХрпНропрпВ. роП рооро▒рпНро▒рпБроорпН рокроЯ роорпАроЯрпНроЯрпЖроЯрпБрокрпНрокрпБ рокрогро┐роХро│рпН роЗро░рогрпНроЯрпБроорпН роЕродро┐роХ роЖро┤родрпНродро┐ро▓ро┐ро░рпБроирпНродрпБ рокропройроЯрпИроХро┐ройрпНро▒рой, роЕродрпЗ роирпЗро░родрпНродро┐ро▓рпН ро╡ро┐. роЪро┐. роЖро░рпН рооро▒рпНро▒рпБроорпН ро░рпЖроГрокрпНроХрпЛроХрпЛ + роЖроХро┐ропро╡рпИ роЖро┤рооро▒рпНро▒ рооро╛родро┐ро░ро┐роХро│ро┐ро▓ро┐ро░рпБроирпНродрпБ рокропройроЯрпИроХро┐ройрпНро▒рой. роЗро▒рпБродро┐ропро╛роХ, роорпБройрпН рокропро┐ро▒рпНроЪро┐ родро░ро╡рпБродрпНродрпКроХрпБрокрпНрокро┐ройрпН роЕро│ро╡ро┐ройрпН родро╛роХрпНроХроорпН роЖро░ро╛ропрокрпНрокроЯрпБроХро┐ро▒родрпБ, роорпЗро▓рпБроорпН рокрпЖро░ро┐роп рокропро┐ро▒рпНроЪро┐ родрпКроХрпБрокрпНрокрпБроХро│рпН роЪро┐ро▒роирпНрод роЪрпЖропро▓рпНродро┐ро▒ройрпИ ро╡ро┐ро│рпИро╡ро┐рокрпНрокродро╛роХ роХрогрпНроЯро▒ро┐ропрокрпНрокроЯрпНроЯрпБро│рпНро│родрпБ.'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_response = bhashini_translate('en','ta', res_llm)\n",
    "translation_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(translation_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
