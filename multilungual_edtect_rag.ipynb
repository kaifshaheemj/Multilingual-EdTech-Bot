{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bhashini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bhashini_translator import Bhashini\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bhashini_translate(sourceLanguage, targetLanguage, text):\n",
    "    bhashini = Bhashini(sourceLanguage, targetLanguage)\n",
    "    return bhashini.translate(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‡Æé‡Æ™‡Øç‡Æ™‡Æü‡Æø ‡Æá‡Æ∞‡ØÅ‡Æï‡Øç‡Æï‡ØÄ‡Æô‡Øç‡Æï?\n"
     ]
    }
   ],
   "source": [
    "print(bhashini_translate('en','ta', 'How are you?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =\"You are an Educational Assistant. The user may ask about various educational topics, technical queries, and other concerns. Respond in a detailed but simple manner. for example the user ask a query like of  this 'What is the boiling point of water?' you should respond in the form of like this 'The boiling point of water is 100¬∞C (212¬∞F) at sea level. This is the temperature at which water changes from a liquid to a gas'.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def LLM(query, content):\n",
    "# Point to the local server\n",
    "    client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "    passLLM = f'Question:{query}\\nContent:{content}'\n",
    "    #print(passLLM)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "    model=\"TheBloke/Llama-2-7B-Chat-GGUF\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an Educational Assistant. The user may ask about various educational topics, technical queries, and other concerns. Respond in a detailed but simple manner. for example the user ask a query like of  this 'What is the boiling point of water?' you should respond in the form of like this 'The boiling point of water is 100¬∞C (212¬∞F) at sea level. This is the temperature at which water changes from a liquid to a gas'.\"},\n",
    "        {\"role\": \"user\", \"content\": passLLM}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    )\n",
    "\n",
    "    print(completion.choices[0].message)\n",
    "    return completion.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45273"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "# Open the PDF file\n",
    "doc = fitz.open(\"D:/Samaaja/Vil-Bert.pdf\")\n",
    "\n",
    "# Extract text from each page\n",
    "pdf_text = \"\"\n",
    "for page in doc:\n",
    "    pdf_text += page.get_text()\n",
    "\n",
    "len(pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_Splitter = RecursiveCharacterTextSplitter(chunk_size= 1000, chunk_overlap=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_Chunks = text_Splitter.create_documents([pdf_text])\n",
    "text_Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [text_Chunk.page_content for text_Chunk in text_Chunks]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without Built-in\n",
    "def recursive_text_splitter(text, max_chunk_size=1000, overlap_size=200):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if len(\" \".join(current_chunk)) >= max_chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = current_chunk[-overlap_size:]  # retain overlap words\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunks = recursive_text_splitter(pdf_text)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import spacy\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(name=\"multilingualedtechrag\")\n",
    "#nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection.add(documents=text, ids = [str(i) for i in range(len(text))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is VilBERT is Parallel or Single Stream? \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = collection.query(query_texts=[question], n_results=4)\n",
    "answer = results['documents'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['network depth for each modality and enables cross-modal connections at different depths.\\nOur model which we call ViLBERT is shown in Fig. 1 and consists of two parallel BERT-style\\nmodels operating over image regions and text segments. Each stream is a series of transformer\\nblocks (TRM) and novel co-attentional transformer layers (Co-TRM) which we introduce to enable\\ninformation exchange between modalities. Given an image I represented as a set of region features\\nv1, . . . , vT and a text input w0, . . . , wT , our model outputs Ô¨Ånal representations hv0, . . . , hvT and\\nhw0, . . . , hwT . Notice that exchange between the two streams is restricted to be between speciÔ¨Åc\\n1Concurrent work [29] modelling language and video sequences takes this approach. See Sec. 5.\\n3\\nVision               &        Language BERT\\n‚Ä¶\\n‚Ñé\"0\\n‚Ñé$%\\n‚Ñé$&\\n‚Ñé$\\'\\n‚Ñé$ùíØ\\n<IMG>\\n‚Ñé)0\\n‚Ñé*%\\n‚Ñé*&\\n‚Ñé*\\'\\n‚Ñé*+\\n<CLS>\\nMan\\nshopping\\nfor\\n<SEP>\\n‚Ä¶\\n‚Ä¶\\n‚Ä¶\\n<MASK>\\n<MASK>\\n<MASK>\\n<MASK>\\nMan shopping\\n(a) Masked multi-modal learning',\n",
       " 'VCR and VQA which have private test sets, we report test results (in parentheses) only for our full\\nmodel. Our full ViLBERT model outperforms task-speciÔ¨Åc state-of-the-art models across all tasks.\\nVQA [3]\\nVCR [25]\\nRefCOCO+ [32]\\nImage Retrieval [26]\\nZS Image Retrieval\\nMethod\\ntest-dev (test-std)\\nQ‚ÜíA\\nQA‚ÜíR\\nQ‚ÜíAR\\nval\\ntestA\\ntestB\\nR1\\nR5\\nR10\\nR1\\nR5\\nR10\\nSOTA\\nDFAF [36]\\n70.22 (70.34)\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nR2C [25]\\n-\\n63.8 (65.1)\\n67.2 (67.3)\\n43.1 (44.0)\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\nMAttNet [33]\\n-\\n-\\n-\\n-\\n65.33\\n71.62\\n56.02\\n-\\n-\\n-\\n-\\n-\\n-\\nSCAN [35]\\n-\\n-\\n-\\n-\\n-\\n-\\n-\\n48.60\\n77.70\\n85.20\\n-\\n-\\n-\\nOurs\\nSingle-Stream‚Ä†\\n65.90\\n68.15\\n68.89\\n47.27\\n65.64\\n72.02\\n56.04\\n-\\n-\\n-\\n-\\n-\\n-\\nSingle-Stream\\n68.85\\n71.09\\n73.93\\n52.73\\n69.21\\n75.32\\n61.02\\n-\\n-\\n-\\n-\\n-\\n-\\nViLBERT‚Ä†\\n68.93\\n69.26\\n71.01\\n49.48\\n68.61\\n75.97\\n58.44\\n45.50\\n76.78\\n85.02\\n0.00\\n0.00\\n0.00\\nViLBERT\\n70.55 (70.92)\\n72.42 (73.3)\\n74.47 (74.6)\\n54.04 (54.8)\\n72.34\\n78.52\\n62.61\\n58.20\\n84.90\\n91.52\\n31.86\\n61.12\\n72.80\\nTask-SpeciÔ¨Åc Baselines. To put our results in context, we present published results of problem-',\n",
       " 'What does ViLBERT learn during pretraining? To get a sense for what ViLBERT learns dur-\\ning Conceptual Caption pretraining, we look at zero-shot caption-based image retreival and some\\nqualitative examples. While zero-shot performance (Tab. 1, right) is signiÔ¨Åcantly lower than the\\nÔ¨Åne-tuned model (31.86 vs 58.20 R1) it performs reasonably without having seen a Flickr30k image\\nor caption (31.86 vs 48.60 R1 for prior SOTA) ‚Äì indicating that ViLBERT has learned a semantically\\nmeaningful alignment between vision and language during pretraining. We also qualitatively inspect\\nthe pretrained ViLBERT model by inputting images and sampling out image-conditioned text. This\\nis essentially image captioning. Note that without Ô¨Åne-tuning the model on clean, human-annotated\\ncaptioning data, the outputs are not likely to be high quality. However, they still serve as a mechanism\\nto inspect what the pretrained model has learned. Fig. 5 shows some of these sampled ‚Äòcaptions‚Äô.',\n",
       " 'Overall, these results demonstrate that our ViLBERT model is able to learn important visual-linguistic\\nrelationships that can be exploited by downstream tasks.\\nEffect of Visual Stream Depth. In Tab. 2 we compare the results transferring from ViLBERT models\\nof varying depths. We consider depth with respect to the number of repeated CO-TRM‚ÜíTRM blocks\\n(shown in a dashed box in Fig. 1) in our model. We Ô¨Ånd that VQA and Image Retrieval tasks\\nbeneÔ¨Åt from greater depth - performance increases monotonically until a layer depth of 6. Likewise,\\nzero-shot image retrieval continues making signiÔ¨Åcant gains as depth increases. In contrast, VCR and\\nRefCOCO+ seem to beneÔ¨Åt from shallower models.\\nBeneÔ¨Åts of Large Training Sets. We also studied the impact of the size of the pretraining dataset.\\nFor this experiment, we take random subsets of 25% and 50% from the conceptual caption dataset,\\nand pretrain and Ô¨Ånetune ViLBERT using the same setup as above. We can see that the accuracy']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ViLBERTisaparallelstreammodelmeaningitconsistsoftwoseparatestreamsofBERTstylemodelsoperatinginparallelEachstreamhasitsownseriesoftransformerblocksandcoattentionaltransformerlayerswhichenableinformationexchangebetweenmodalitiesTheinputimageisrepresentedasasetofregionfeaturesandtheinputtextisrepresentedasasequenceofwordsThefinalrepresentationsfrombothstreamsarecombinedandthemodeloutputsanalrepresentationTheViLBERTmodellearnsimportantvisuallinguisticrelationshipsduringpretrainingasdemonstratedbyitsperformanceonzeroshotcaptionbasedimageretrievaltasksThemodelsabilitytotransferknowledgetodownstreamtasksisevidentinitsperformanceacrossvarioustasksincludingVQAImageRetrievalandRefCOCOTheeffectofvisualstreamdepthisalsostudiedanditisfoundthatbothVQAandImageRetrievaltasksbenetfromgreaterdepthwhileVCRandRefCOCOseemtobenetfromshallowermodelsFinallytheimpactofthesizeofthepretrainingdatasetisexploredanditisfoundthatlargertrainingsetsresultinbetterperformance'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_and_extract_words(text):\n",
    "    words = re.findall(r'[A-Za-z]+', text)\n",
    "    return ' '.join(words)\n",
    "\n",
    "def clean_segments_and_print(answer):\n",
    "    cleaned_content = []\n",
    "\n",
    "    for segment in answer:\n",
    "        cleaned_segment = clean_and_extract_words(segment)\n",
    "        cleaned_content.append(cleaned_segment)\n",
    "    seg = \"\"\n",
    "    for i, segment in enumerate(cleaned_content, start=1):\n",
    "        #print(f\"Segment {i}:\")\n",
    "        seg += segment\n",
    "        #print(segment)\n",
    "        #print(\"\\n\")\n",
    "    return seg\n",
    "\n",
    "cleaned_text = clean_segments_and_print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"ViLBERT is a parallel stream model, meaning it consists of two separate streams of BERT-style models operating in parallel. Each stream has its own series of transformer blocks and co-attentional transformer layers, which enable information exchange between modalities. The input image is represented as a set of region features, and the input text is represented as a sequence of words. The final representations from both streams are combined, and the model outputs a Ô¨Ånal representation.\\nThe ViLBERT model learns important visual-linguistic relationships during pretraining, as demonstrated by its performance on zero-shot caption-based image retrieval tasks. The model's ability to transfer knowledge to downstream tasks is evident in its performance across various tasks, including VQA, Image Retrieval, and RefCOCO+. The effect of visual stream depth is also studied, and it is found that both VQA and Image Retrieval tasks beneÔ¨Åt from greater depth, while VCR and RefCOCO+ seem to beneÔ¨Åt from shallower models. Finally, the impact of the size of the pretraining dataset is explored, and it is found that larger training sets result in better performance.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "response = LLM(question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_llm =\"ViLBERT is a parallel stream model, meaning it consists of two separate streams of BERT-style models operating in parallel. Each stream has its own series of transformer blocks and co-attentional transformer layers, which enable information exchange between modalities. The input image is represented as a set of region features, and the input text is represented as a sequence of words. The final representations from both streams are combined, and the model outputs a Ô¨Ånal representation.\\nThe ViLBERT model learns important visual-linguistic relationships during pretraining, as demonstrated by its performance on zero-shot caption-based image retrieval tasks. The model's ability to transfer knowledge to downstream tasks is evident in its performance across various tasks, including VQA, Image Retrieval, and RefCOCO+. The effect of visual stream depth is also studied, and it is found that both VQA and Image Retrieval tasks beneÔ¨Åt from greater depth, while VCR and RefCOCO+ seem to beneÔ¨Åt from shallower models. Finally, the impact of the size of the pretraining dataset is explored, and it is found that larger training sets result in better performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ViLBERT is a parallel stream model, meaning it consists of two separate streams of BERT-style models operating in parallel. Each stream has its own series of transformer blocks and co-attentional transformer layers, which enable information exchange between modalities. The input image is represented as a set of region features, and the input text is represented as a sequence of words. The final representations from both streams are combined, and the model outputs a Ô¨Ånal representation.\\nThe ViLBERT model learns important visual-linguistic relationships during pretraining, as demonstrated by its performance on zero-shot caption-based image retrieval tasks. The model's ability to transfer knowledge to downstream tasks is evident in its performance across various tasks, including VQA, Image Retrieval, and RefCOCO+. The effect of visual stream depth is also studied, and it is found that both VQA and Image Retrieval tasks beneÔ¨Åt from greater depth, while VCR and RefCOCO+ seem to beneÔ¨Åt from shallower models. Finally, the impact of the size of the pretraining dataset is explored, and it is found that larger training sets result in better performance\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VILBERT ‡Æé‡Æ©‡Øç‡Æ™‡Æ§‡ØÅ ‡Æí‡Æ∞‡ØÅ ‡Æá‡Æ£‡Øà‡ÆØ‡Ææ‡Æ© ‡Æ∏‡Øç‡Æü‡Øç‡Æ∞‡ØÄ‡ÆÆ‡Øç ‡ÆÆ‡Ææ‡Æ§‡Æø‡Æ∞‡Æø‡ÆØ‡Ææ‡Æï‡ØÅ‡ÆÆ‡Øç, ‡ÆÖ‡Æ§‡Ææ‡Æµ‡Æ§‡ØÅ ‡Æá‡Æ§‡ØÅ ‡Æá‡Æ£‡Øà‡ÆØ‡Ææ‡Æï ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ™‡Æü‡ØÅ‡ÆÆ‡Øç BERT-‡Æ™‡Ææ‡Æ£‡Æø ‡ÆÆ‡Ææ‡Æ§‡Æø‡Æ∞‡Æø‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡Æá‡Æ∞‡Æ£‡Øç‡Æü‡ØÅ ‡Æ§‡Æ©‡Æø‡Æ§‡Øç‡Æ§‡Æ©‡Æø ‡Æ∏‡Øç‡Æü‡Øç‡Æ∞‡ØÄ‡ÆÆ‡Øç‡Æï‡Æ≥‡Øà‡Æï‡Øç ‡Æï‡Øä‡Æ£‡Øç‡Æü‡ØÅ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ. ‡Æí‡Æµ‡Øç‡Æµ‡Øä‡Æ∞‡ØÅ ‡Æ∏‡Øç‡Æü‡Øç‡Æ∞‡ØÄ‡ÆÆ‡Øç ‡ÆÖ‡Æ§‡Æ©‡Øç ‡Æö‡Øä‡Æ®‡Øç‡Æ§ ‡Æ§‡Øä‡Æü‡Æ∞‡Øç ‡ÆÆ‡Æø‡Æ©‡Øç‡ÆÆ‡Ææ‡Æ±‡Øç‡Æ±‡Æø ‡Æ§‡Øä‡Æï‡ØÅ‡Æ§‡Æø‡Æï‡Æ≥‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æá‡Æ£‡Øà ‡Æï‡Æµ‡Æ©‡ÆÆ‡Øç ‡ÆÆ‡Æø‡Æ©‡Øç‡ÆÆ‡Ææ‡Æ±‡Øç‡Æ±‡Æø ‡ÆÖ‡Æü‡ØÅ‡Æï‡Øç‡Æï‡ØÅ‡Æï‡Æ≥‡Øà‡Æï‡Øç ‡Æï‡Øä‡Æ£‡Øç‡Æü‡ØÅ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ, ‡Æá‡Æ§‡ØÅ ‡ÆÆ‡ØÅ‡Æ±‡Øà‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡Æá‡Æü‡Øà‡ÆØ‡Æø‡Æ≤‡Øç ‡Æ§‡Æï‡Æµ‡Æ≤‡Øç ‡Æ™‡Æ∞‡Æø‡ÆÆ‡Ææ‡Æ±‡Øç‡Æ±‡Æ§‡Øç‡Æ§‡Øà ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ. ‡Æâ‡Æ≥‡Øç‡Æ≥‡ØÄ‡Æü‡Øç‡Æü‡ØÅ ‡Æ™‡Æü‡ÆÆ‡Øç ‡Æ™‡Æø‡Æ∞‡Ææ‡Æ®‡Øç‡Æ§‡Æø‡ÆØ ‡ÆÖ‡ÆÆ‡Øç‡Æö‡Æô‡Øç‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡Æ§‡Øä‡Æï‡ØÅ‡Æ™‡Øç‡Æ™‡Ææ‡Æï ‡Æï‡ØÅ‡Æ±‡Æø‡Æ™‡Øç‡Æ™‡Æø‡Æü‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ, ‡ÆÆ‡Øá‡Æ≤‡ØÅ‡ÆÆ‡Øç ‡Æâ‡Æ≥‡Øç‡Æ≥‡ØÄ‡Æü‡Øç‡Æü‡ØÅ ‡Æâ‡Æ∞‡Øà ‡Æö‡Øä‡Æ±‡Øç‡Æï‡Æ≥‡Æø‡Æ©‡Øç ‡Æµ‡Æ∞‡Æø‡Æö‡Øà‡ÆØ‡Ææ‡Æï ‡Æï‡ØÅ‡Æ±‡Æø‡Æ™‡Øç‡Æ™‡Æø‡Æü‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ. ‡Æá‡Æ∞‡Æ£‡Øç‡Æü‡ØÅ ‡Æ®‡ØÄ‡Æ∞‡Øã‡Æü‡Øà‡Æï‡Æ≥‡Æø‡Æ≤‡Æø‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ‡ÆÆ‡Øç ‡Æá‡Æ±‡ØÅ‡Æ§‡Æø ‡Æ™‡Æø‡Æ∞‡Æ§‡Æø‡Æ®‡Æø‡Æ§‡Æø‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æá‡Æ£‡Øà‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ©‡Øç‡Æ±‡Æ©, ‡ÆÆ‡Øá‡Æ≤‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡Ææ‡Æ§‡Æø‡Æ∞‡Æø ‡Æí‡Æ∞‡ØÅ ‡Æá‡Æ±‡ØÅ‡Æ§‡Æø ‡Æ™‡Æø‡Æ∞‡Æ§‡Æø‡Æ®‡Æø‡Æ§‡Æø‡Æ§‡Øç‡Æ§‡ØÅ‡Æµ‡Æ§‡Øç‡Æ§‡Øà ‡Æµ‡ØÜ‡Æ≥‡Æø‡ÆØ‡Æø‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ. VILBERT ‡ÆÆ‡Ææ‡Æ§‡Æø‡Æ∞‡Æø ‡ÆÆ‡ØÅ‡Æ©‡Øç ‡Æ™‡ÆØ‡Æø‡Æ±‡Øç‡Æö‡Æø‡ÆØ‡Æø‡Æ©‡Øç ‡Æ™‡Øã‡Æ§‡ØÅ ‡ÆÆ‡ØÅ‡Æï‡Øç‡Æï‡Æø‡ÆØ‡ÆÆ‡Ææ‡Æ© ‡Æï‡Ææ‡Æü‡Øç‡Æö‡Æø-‡ÆÆ‡Øä‡Æ¥‡Æø‡ÆØ‡Æø‡ÆØ‡Æ≤‡Øç ‡Æâ‡Æ±‡Æµ‡ØÅ‡Æï‡Æ≥‡Øà‡Æï‡Øç ‡Æï‡Æ±‡Øç‡Æ±‡ØÅ‡Æï‡Øç‡Æï‡Øä‡Æ≥‡Øç‡Æï‡Æø‡Æ±‡Æ§‡ØÅ, ‡Æá‡Æ§‡ØÅ ‡Æ™‡ØÇ‡Æú‡Øç‡Æú‡Æø‡ÆØ-‡Æ∑‡Ææ‡Æü‡Øç ‡Æ§‡Æ≤‡Øà‡Æ™‡Øç‡Æ™‡ØÅ-‡ÆÖ‡Æü‡Æø‡Æ™‡Øç‡Æ™‡Æü‡Øà‡ÆØ‡Æø‡Æ≤‡Ææ‡Æ© ‡Æ™‡Æü ‡ÆÆ‡ØÄ‡Æü‡Øç‡Æü‡ØÜ‡Æü‡ØÅ‡Æ™‡Øç‡Æ™‡ØÅ ‡Æ™‡Æ£‡Æø‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡ÆÖ‡Æ§‡Æ©‡Øç ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ§‡Æø‡Æ±‡Æ©‡Ææ‡Æ≤‡Øç ‡Æ®‡Æø‡Æ∞‡ØÇ‡Æ™‡Æø‡Æï‡Øç‡Æï‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡ØÅ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ. ‡Æï‡ØÄ‡Æ¥‡Øç‡Æ®‡Æø‡Æ≤‡Øà‡Æ™‡Øç ‡Æ™‡Æ£‡Æø‡Æï‡Æ≥‡ØÅ‡Æï‡Øç‡Æï‡ØÅ ‡ÆÖ‡Æ±‡Æø‡Æµ‡Øà ‡ÆÆ‡Ææ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡ÆÆ‡Ææ‡Æ§‡Æø‡Æ∞‡Æø‡ÆØ‡Æø‡Æ©‡Øç ‡Æ§‡Æø‡Æ±‡Æ©‡Øç VQA, ‡Æ™‡Æü ‡ÆÆ‡ØÄ‡Æü‡Øç‡Æü‡ØÜ‡Æü‡ØÅ‡Æ™‡Øç‡Æ™‡ØÅ ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æ∞‡ØÜ‡ÆÉ‡Æ™‡Øç‡Æï‡Øã‡Æï‡Øã + ‡Æâ‡Æ≥‡Øç‡Æ≥‡Æø‡Æü‡Øç‡Æü ‡Æ™‡Æ≤‡Øç‡Æµ‡Øá‡Æ±‡ØÅ ‡Æ™‡Æ£‡Æø‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡ÆÖ‡Æ§‡Æ©‡Øç ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ§‡Æø‡Æ±‡Æ©‡Æø‡Æ≤‡Øç ‡Æ§‡ØÜ‡Æ≥‡Æø‡Æµ‡Ææ‡Æï‡Æ§‡Øç ‡Æ§‡ØÜ‡Æ∞‡Æø‡Æï‡Æø‡Æ±‡Æ§‡ØÅ. ‡Æï‡Ææ‡Æü‡Øç‡Æö‡Æø ‡Æ∏‡Øç‡Æü‡Øç‡Æ∞‡ØÄ‡ÆÆ‡Øç ‡ÆÜ‡Æ¥‡Æ§‡Øç‡Æ§‡Æø‡Æ©‡Øç ‡Æµ‡Æø‡Æ≥‡Øà‡Æµ‡ØÅ‡ÆÆ‡Øç ‡ÆÜ‡ÆØ‡Øç‡Æµ‡ØÅ ‡Æö‡ØÜ‡ÆØ‡Øç‡ÆØ‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ, ‡ÆÆ‡Øá‡Æ≤‡ØÅ‡ÆÆ‡Øç ‡Æµ‡Æø. ‡Æï‡Øç‡ÆØ‡ØÇ. ‡Æè ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æ™‡Æü ‡ÆÆ‡ØÄ‡Æü‡Øç‡Æü‡ØÜ‡Æü‡ØÅ‡Æ™‡Øç‡Æ™‡ØÅ ‡Æ™‡Æ£‡Æø‡Æï‡Æ≥‡Øç ‡Æá‡Æ∞‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç ‡ÆÖ‡Æ§‡Æø‡Æï ‡ÆÜ‡Æ¥‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Æø‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ ‡Æ™‡ÆØ‡Æ©‡Æü‡Øà‡Æï‡Æø‡Æ©‡Øç‡Æ±‡Æ©, ‡ÆÖ‡Æ§‡Øá ‡Æ®‡Øá‡Æ∞‡Æ§‡Øç‡Æ§‡Æø‡Æ≤‡Øç ‡Æµ‡Æø. ‡Æö‡Æø. ‡ÆÜ‡Æ∞‡Øç ‡ÆÆ‡Æ±‡Øç‡Æ±‡ØÅ‡ÆÆ‡Øç ‡Æ∞‡ØÜ‡ÆÉ‡Æ™‡Øç‡Æï‡Øã‡Æï‡Øã + ‡ÆÜ‡Æï‡Æø‡ÆØ‡Æµ‡Øà ‡ÆÜ‡Æ¥‡ÆÆ‡Æ±‡Øç‡Æ± ‡ÆÆ‡Ææ‡Æ§‡Æø‡Æ∞‡Æø‡Æï‡Æ≥‡Æø‡Æ≤‡Æø‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ ‡Æ™‡ÆØ‡Æ©‡Æü‡Øà‡Æï‡Æø‡Æ©‡Øç‡Æ±‡Æ©. ‡Æá‡Æ±‡ØÅ‡Æ§‡Æø‡ÆØ‡Ææ‡Æï, ‡ÆÆ‡ØÅ‡Æ©‡Øç ‡Æ™‡ÆØ‡Æø‡Æ±‡Øç‡Æö‡Æø ‡Æ§‡Æ∞‡Æµ‡ØÅ‡Æ§‡Øç‡Æ§‡Øä‡Æï‡ØÅ‡Æ™‡Øç‡Æ™‡Æø‡Æ©‡Øç ‡ÆÖ‡Æ≥‡Æµ‡Æø‡Æ©‡Øç ‡Æ§‡Ææ‡Æï‡Øç‡Æï‡ÆÆ‡Øç ‡ÆÜ‡Æ∞‡Ææ‡ÆØ‡Æ™‡Øç‡Æ™‡Æü‡ØÅ‡Æï‡Æø‡Æ±‡Æ§‡ØÅ, ‡ÆÆ‡Øá‡Æ≤‡ØÅ‡ÆÆ‡Øç ‡Æ™‡ØÜ‡Æ∞‡Æø‡ÆØ ‡Æ™‡ÆØ‡Æø‡Æ±‡Øç‡Æö‡Æø ‡Æ§‡Øä‡Æï‡ØÅ‡Æ™‡Øç‡Æ™‡ØÅ‡Æï‡Æ≥‡Øç ‡Æö‡Æø‡Æ±‡Æ®‡Øç‡Æ§ ‡Æö‡ØÜ‡ÆØ‡Æ≤‡Øç‡Æ§‡Æø‡Æ±‡Æ©‡Øà ‡Æµ‡Æø‡Æ≥‡Øà‡Æµ‡Æø‡Æ™‡Øç‡Æ™‡Æ§‡Ææ‡Æï ‡Æï‡Æ£‡Øç‡Æü‡Æ±‡Æø‡ÆØ‡Æ™‡Øç‡Æ™‡Æü‡Øç‡Æü‡ØÅ‡Æ≥‡Øç‡Æ≥‡Æ§‡ØÅ.'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translation_response = bhashini_translate('en','ta', res_llm)\n",
    "translation_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(translation_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
